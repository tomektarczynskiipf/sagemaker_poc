{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "887d7724-a8bf-4acb-9e41-31103fc4e9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import sagemaker.session\n",
    "import os\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "from sagemaker.transformer import Transformer\n",
    "from sagemaker.workflow.steps import TransformStep\n",
    "from sagemaker.inputs import TransformInput\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "\n",
    "from scripts.functions import *\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "settings = read_settings('scripts/settings.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cad5b983-ecfa-4a47-bcbc-2fe9a1ccf9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://sagemaker-bucket-ds/01-churn/v1/data/test/test.csv\n",
      "delete: s3://sagemaker-bucket-ds/01-churn/v1/data/train/train.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 rm s3://sagemaker-bucket-ds/01-churn/ --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5f07cdf-a77c-4c1a-8a58-e73df6c4b04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "pipeline_session = PipelineSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce529c68-e253-43ee-bb57-99851b0f2d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/processing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/processing.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_random_dataframe_with_params(n_rows, n_cols, params, seed=None):\n",
    "    \"\"\"\n",
    "    Create a DataFrame with random values and an additional binary target column based on the sum of products of values and parameters.\n",
    "    \n",
    "    Parameters:\n",
    "    - n_rows: int, number of rows in the DataFrame\n",
    "    - n_cols: int, number of columns in the DataFrame\n",
    "    - params: list or array-like, parameters for each column\n",
    "    - seed: int, random seed for reproducibility (default is None)\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with shape (n_rows, n_cols+1) where the last column is a binary target based on the sum of products.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "    if len(params) != n_cols:\n",
    "        raise ValueError(\"The length of params must be equal to the number of columns (n_cols).\")\n",
    "    \n",
    "    data = np.random.rand(n_rows, n_cols)\n",
    "    df = pd.DataFrame(data, columns=[f'col_{i+1}' for i in range(n_cols)])\n",
    "    \n",
    "    # Calculate the sum_product column\n",
    "    df['sum_product'] = np.dot(df.values, params) + 0.5\n",
    "    \n",
    "    # Calculate the target column\n",
    "    df['target'] = (np.random.rand(n_rows) < df['sum_product']).astype(int)\n",
    "    \n",
    "    # Drop the sum_product column\n",
    "    df = df.drop(columns=['sum_product'])\n",
    "\n",
    "    # Move target column to the first position\n",
    "    columns = ['target'] + [col for col in df.columns if col != 'target']\n",
    "    df = df[columns]    \n",
    "    \n",
    "    return df\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    settings = os.environ\n",
    "    \n",
    "    # Create data\n",
    "    params = [-1, -1, -0.5, 0, 0, 0.5, 1, 1]\n",
    "    df = create_random_dataframe_with_params(n_rows = 100000, n_cols = 8, params = params, seed = 42)\n",
    "    \n",
    "\n",
    "    train, temp = train_test_split(df, test_size=0.4, random_state=42)\n",
    "    test, valid = train_test_split(temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    train = train[['target', \"col_1\", \"col_2\", \"col_3\", \"col_6\", \"col_7\", \"col_8\"]]\n",
    "    test = test[['target', \"col_1\", \"col_2\", \"col_3\", \"col_6\", \"col_7\", \"col_8\"]]\n",
    "    valid = valid[['target', \"col_1\", \"col_2\", \"col_3\", \"col_6\", \"col_7\", \"col_8\"]]\n",
    "    inference_train = train[[\"col_1\", \"col_2\", \"col_3\", \"col_6\", \"col_7\", \"col_8\"]]\n",
    "\n",
    "    train_path = os.path.join(settings[\"preprocessing_output_train\"], \"train.csv\")\n",
    "    train.to_csv(train_path, index=False, float_format='%.5f')\n",
    "\n",
    "    test_path = os.path.join(settings[\"preprocessing_output_test\"], \"test.csv\")\n",
    "    test.to_csv(test_path, index=False, float_format='%.5f')\n",
    "\n",
    "    valid_path = os.path.join(settings[\"preprocessing_output_valid\"], \"valid.csv\")\n",
    "    valid.to_csv(valid_path, index=False, float_format='%.5f')\n",
    "\n",
    "    inference_train_path = os.path.join(settings[\"preprocessing_output_inference_train\"], \"inference_train.csv\")\n",
    "    valid.to_csv(inference_train_path, index=False, float_format='%.5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c4d6553-a047-4e01-8444-781739786e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n"
     ]
    }
   ],
   "source": [
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=\"0.23-1\",\n",
    "    instance_type='ml.t3.medium',\n",
    "    instance_count=1,\n",
    "    base_job_name=settings['preprocessing_job_name'],\n",
    "    sagemaker_session=pipeline_session,\n",
    "    role=role,\n",
    "    env=settings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf8825d4-a4ce-479c-88d2-580066ef2db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:332: UserWarning: Running within a PipelineSession, there will be No Wait, No Logs, and No Job being started.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_s3_path = os.path.join(\"s3://\",settings['bucket_name'],settings['project_path_s3'],'data','train')\n",
    "test_s3_path = os.path.join(\"s3://\",settings['bucket_name'],settings['project_path_s3'],'data','test')\n",
    "valid_s3_path = os.path.join(\"s3://\",settings['bucket_name'],settings['project_path_s3'],'data','valid')\n",
    "inference_train_s3_path = os.path.join(\"s3://\",settings['bucket_name'],settings['project_path_s3'],'data','inference_train')\n",
    "\n",
    "processor_args = sklearn_processor.run(\n",
    "    inputs=[],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"train\",\n",
    "            source=settings[\"preprocessing_output_train\"],\n",
    "            destination=train_s3_path),\n",
    "        ProcessingOutput(\n",
    "            output_name=\"test\",\n",
    "            source=settings[\"preprocessing_output_test\"],\n",
    "            destination=test_s3_path),\n",
    "        ProcessingOutput(\n",
    "            output_name=\"valid\",\n",
    "            source=settings[\"preprocessing_output_valid\"],\n",
    "            destination=valid_s3_path),\n",
    "        ProcessingOutput(\n",
    "            output_name=\"inference_train\",\n",
    "            source=settings[\"preprocessing_output_inference_train\"],\n",
    "            destination=inference_train_s3_path)        \n",
    "\n",
    "    ],\n",
    "    code=\"scripts/processing.py\",\n",
    ") \n",
    "\n",
    "step_process = ProcessingStep(\n",
    "    name=settings[\"preprocessing_step_name\"],\n",
    "    step_args=processor_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b5fdc19-2a5c-4252-a9e2-ff931aac3207",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = {\n",
    "    'mlflow_arn': settings['mlflow_arn'],\n",
    "    'mlflow_experiment_name': settings['mlflow_experiment_name'],\n",
    "    'mlflow_final_model_name': 'final-model2',\n",
    "    'mlflow_model_name': settings['mlflow_model_name']\n",
    "}\n",
    "\n",
    "sklearn = SKLearn(\n",
    "    entry_point='train.py', # The file with the training code\n",
    "    source_dir='scripts', # The folder with the training code\n",
    "    framework_version='1.2-1', # Version of SKLearn which will be used\n",
    "    instance_type='ml.m5.large', # Instance type that wil be used\n",
    "    role=role, # Role that will be used during execution\n",
    "    sagemaker_session=pipeline_session, \n",
    "    base_job_name=settings['training_job_name'], # Name of the training job. Timestamp will be added as suffix\n",
    "    environment = environment\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "819e22f7-f540-456f-beee-246fa3bd5b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = sklearn.fit({\"train\": step_process.properties.ProcessingOutputConfig.Outputs['train'].S3Output.S3Uri})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b2e1648-8685-4667-9914-b3f2718789c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_train = TrainingStep(\n",
    "    name=settings[\"training_step_name\"],\n",
    "    step_args = train_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b02de8d-9479-4290-a153-3bbcf75c1472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the SKLearnModel\n",
    "sklearn_model = SKLearnModel(\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    entry_point='inference.py', # The file with the training code\n",
    "    source_dir=\"scripts\", # The folder with the training code\n",
    "    role=role,\n",
    "    framework_version='1.2-1',  # Replace with the appropriate sklearn version\n",
    "    sagemaker_session=pipeline_session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11fbe9b4-c799-4ae6-b147-5bbd019a3778",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_create_model = ModelStep(\n",
    "   name=settings[\"modelcreate_step_name\"],\n",
    "   step_args=sklearn_model.create(instance_type=\"ml.m5.large\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70af832a-60b1-4681-8261-a86e1da75c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_output_path = os.path.join(\"s3://\",settings['bucket_name'],settings['project_path_s3'],'output','inference_train')\n",
    "\n",
    "transformer = Transformer(\n",
    "    model_name=step_create_model.properties.ModelName,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    instance_count=1,\n",
    "    output_path=transformer_output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8a963a4-8585-4a0e-91da-1648dab54d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_transform = TransformStep(\n",
    "    name=settings[\"transformer_step_name\"],\n",
    "    transformer=transformer,\n",
    "    inputs=TransformInput(\n",
    "        data=step_process.properties.ProcessingOutputConfig.Outputs['inference_train'].S3Output.S3Uri,\n",
    "        content_type='text/csv', # It is neccessary because csv is not default format\n",
    "        split_type='Line' # Each line equals one observation)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9aab9e12-dd93-477f-8856-10497a208653",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_name = f\"01-churn-deploy-model\"\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    steps=[step_process, step_train, step_create_model, step_transform],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d48f7f59-4a06-41d8-b9a6-074f16e8ec96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TransformJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:eu-west-1:211125740051:pipeline/01-churn-deploy-model',\n",
       " 'ResponseMetadata': {'RequestId': '1ab3f845-e6e2-4c5a-9698-8b9d17b207c6',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '1ab3f845-e6e2-4c5a-9698-8b9d17b207c6',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '89',\n",
       "   'date': 'Mon, 15 Jul 2024 11:22:58 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abe111fc-2f8c-47d8-9d81-5d4ebad2fbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
